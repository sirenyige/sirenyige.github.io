---
title: 信息论简介
date: 2020-05-20 23:20:21
categories:
- 机器学习
tags:
- 信息论

---

# 信息论简介

香农在《通信的数学理论》中开宗明义：“通信的基本问题是在一点精确地或近似地复现在另一点所选取的消息。消息通常有意义，即根据某种体系，消息本身指向或关联着物理上或概念上的特定实体。但消息的语义含义与工程问题无关，重要的问题是一条消息来自于一个所有可能的消息的集合。”

<!-- more -->

## 一、信息熵

信息论使用“信息熵”的概念，对单个信源的信息量和通信中传递信息的数量与效率等问题做出了解释，并在世界的不确定性和信息的可测量性之间搭建起一座桥梁。

**熵的本质即是一个系统内在的混乱程度。**

在信息论中，如果事件$A$ 发生的概率为 $p(A)$，则这个事件的自信息量的定义为
$$
h(A)=-log_{2}p(A)
$$
根据单个事件的自信息量可以计算包含多个符号的信源的信息熵。**信源的信息熵是信源可能发出的各个符号的自信息量在信源构成的概率空间上的统计平均值**。如果一个离散信源 $X$ 包含 $n$ 个符号，每个符号 $a_i$ 的取值为 $p(a_i)$，则 $X$ 的信源熵为
$$
H(X)=-\sum_{i=1}^{n} p\left(a_{i}\right) \log_{2} p\left(a_{i}\right)
$$
信源熵描述了信源每发送一个符号所提供的平均信息量，是信源总体信息测度的均值。当信源中的每个符号的取值概率相等时，信源熵取到最大值 $log_{2}n$，意味着信源的随机程度最高。



## 二、条件熵

在概率论中有条件概率的概念，将条件概率扩展到信息论中，就可以得到条件熵。如果两个信源之间具有相关性，那么在已知其中一个信源 $X$ 的条件下，另一个信源 $Y$ 的信源熵就会减小。条件熵 $H(Y∣X)$ 表示的是在已知随机变量 $X$ 的条件下另一个随机变量 $Y$ 的不确定性，也就是在给定 $X$ 时，根据 $Y$ 的条件概率计算出的熵再对 $X$ 求解数学期望：

$$
\begin{align}
H(Y | X)&=\sum_{i=1}^{n} p\left(x_{i}\right) H\left(Y | X=x_{i}\right) \\\\
&=-\sum_{i=1}^{n} p\left(x_{i}\right) \sum_{j=1}^{m} p\left(y_{j} | x_{i}\right) \log_{2} p\left(y_{j} | x_{i}\right) \\\\
&=-\sum_{i=1}^{n} \sum_{j=1}^{m} p\left(x_{i}, y_{j}\right) \log_{2} p\left(y_{j} | x_{i}\right)
\end{align}
$$

条件熵的意义在于先按照变量 X 的取值对变量 Y 进行了一次分类，对每个分出来的类别计算其单独的信息熵，再将每个类的信息熵按照 X 的分布计算其数学期望。

## 三、互信息

定义了条件信息熵后，就可以进一步得到互信息的概念
$$
I(X ; Y)=H(Y)-H(Y | X)
$$
**互信息等于 $Y$ 的信源熵减去已知 $X$ 时 $Y$ 的条件熵，即由 $X$ 提供的关于 $Y$ 的不确定性的消除，也可以看成是 $X$ 给 $Y$ 带来的信息增益**。互信息这个名称在通信领域经常使用，信息增益则在机器学习领域中经常使用，两者的本质是一样的。

在机器学习中，信息增益常常被用于分类特征的选择。

对于给定的训练数据集 $Y$，$H(Y)$ 表示在未给定任何特征时，对训练集进行分类的不确定性；$H(Y∣X)$ 则表示了使用特征 $X$ 对训练集 $Y$ 进行分类的不确定性。信息增益表示的就是特征 $X$ 带来的对训练集 $Y$ 分类不确定性的减少程度，也就是特征 $X$ 对训练集 $Y$ 的区分度。显然，信息增益更大的特征具有更强的分类能力。但信息增益的值很大程度上依赖于数据集的信息熵 $H(Y)$，因而并不具有绝对意义。为解决这一问题，研究者又提出了信息增益比的概念，并将其定义为:
$$
g(X, Y)=I(X ; Y) / H(Y)
$$

## 三、交叉熵

用分布 P 的最佳信息传递方式来传达分布 Q 中随机抽选的一个事件，所需的平均信息长度为交叉熵，表达为
$$
H_{P}(Q)=\int_{x} q(x) \log \frac{1}{p(x)} d x
$$


## 四、Kullback-Leibler 散度

另一个在机器学习中经常使用的信息论概念叫作“Kullback-Leibler 散度”，简称 KL 散度。**KL 散度是描述两个概率分布 P 和 Q 之间的差异的一种方法**。其表示用分布 Q 的最佳信息传递方式来传达分布 P，比用分布 P 自己的最佳信息传递方式来传达分布 P，平均多耗费的信息长度为 KL 散度，**KL 散度是对额外信息量的衡量**。其定义为
$$
\begin{align}
D_{K L}(P \| Q)&=H_{Q}(P)-H(P)\\\\
&= \sum_{x} p(x) \log \frac{1}{q(x)}-\sum_{x} p(x) \log \frac{1}{p(x)}\\\\
&=\sum_{i=1}^{n} p\left(x_{i}\right) \log_{2} \frac{p\left(x_{i}\right)}{q\left(x_{i}\right)}
\end{align}
$$

给定一个信源，其符号的概率分布为 $P(X)$，就可以设计一种针对 $P(X)$ 的最优编码，使得表示该信源所需的平均比特数最少（等于该信源的信源熵）。可是当信源的符号集合不变，而符合的概率分布变为 $Q(X)$ 时，再用概率分布 $P(X)$ 的最优编码对符合分布 $Q(X)$ 的符号编码，此时编码结果的字符数就会比最优值多一些比特。KL 散度就是用来衡量这种情况下平均每个字符多用的比特数，也可以表示两个分布之间的距离。

$D_{K L}(P \| Q)$中涉及了两个分布

- 要传达的信息来自分布P
- 信息传递方式由分布Q决定

由 KL 散度的公式可知，分布 P 里可能性越大的事件，对$D_{K L}(P \| Q)$ 影响力越大。如果想让$D_{K L}(P \| Q)$尽量小，就要优先关注分布 P 里的常见事件（假设为 x），确保它们在分布 Q 里不是特别罕见。

因为一旦事件 x 在分布 Q 里罕见，意味着在设计分布 Q 的信息传递方式时，没有着重优化传递 x 的成本，传达事件 x 所需的成本，log(1/q(x)) 会特别大。所以，当这一套传递方式被用于传达分布 P 的时候，我们会发现，传达常见事件需要的成本特别大，整体成本也就特别大。类似地，想让 $D_{K L}(Q \| P)$ 特别小，就要优先考虑分布 Q 里那些常见的事件了。

## 五、KL散度的不对称性

假设存在一个真实分布 P，由两个高斯分布混合而成，用蓝线表示。

<img src="/信息论简介/KL散度1.jpeg" alt="KL散度1" style="zoom: 33%;" />

在不知道分布 P 的信息的情况下，我们做出了一个常见的假设：假设数据符合高斯分布。当我们尝试用一个普通的高斯分布 Q 来近似分布 P，换言之，尝试让 Q 尽量「贴近」P 的时候，可以选择的目标函数有：

$$
1、Q^{\*}=\arg \min D_{K L}(P \| Q)\\\\
2、Q^{\*}=\arg \min D_{K L}(Q \| P)
$$

选择不同的目标函数，会产生完全不同的 Q。

<img src="/信息论简介/KL散度2.jpeg" alt="KL散度2" style="zoom:50%;" />

1、如果我们选择目标函数 1，结果会像左图一样。在优化过程中，重要的是分布 P 中的*常见事件*，也就是蓝线的两峰，我们要优先确保它们在分布 Q 里不是特别罕见（信息长度不是特别长）。由于分布 P 里有两个峰值区域，分布 Q 无法偏向任何一个峰值，拉锯的结果是，Q 选择了横亘在分布 P 两个峰值中间。

2、如果我们选择目标函数 2，结果会像右图一样，重要的是分布 P 中的*罕见事件*（信息长度特别长的那些事件），也就是蓝线的谷底，我们优先确保它们在分布 Q 里不是特别常见。左图里那种，分布 Q 横亘在分布 P 两个峰值中间，是我们最不希望发生的、KL 散度格外大的情况。相反，只有一个峰值的分布 Q 最终会选择贴合分布 P 两个峰值区域中的任意一个。

[1]: https://www.jiqizhixin.com/articles/0224	"如何理解KL散度的不对称性"

