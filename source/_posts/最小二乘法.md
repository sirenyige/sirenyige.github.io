---
title: 最小二乘法
date: 2020-06-11 00:05:22
categories:
- 机器学习
tags:
- 最小二乘法
---

# 最小二乘法

线性回归中，预测输出 $f(x)$ 和真实输出 $y$ 之间的误差是关注的核心指标。这一误差是以均方误差来定义的。当线性回归的模型为二维平面上的直线时，均方误差就是预测输出和真实输出之间的欧几里得距离，也就是两点间向量的 L2 范数。而以使均方误差取得最小值为目标的模型求解方法就是最小二乘法，其表达式可以写成：
$$
\begin{aligned}
w^{*}&= \mathop{\arg\min_{w}}\sum_{k=1}{\left ( w^{T}x_{k}-y_{k}\right )}^{2}\\\\
&=\mathop{\arg\min_{w}}\sum_{k=1}{\left \| y_{k}-w^{T}x_{k}\right \|}^{2}
\end{aligned}
$$
式中每个 $x_k$ 代表训练集中的一个样本。

<!-- more -->

## 一、为什么使均方误差最小化的参数就是和训练样本匹配的最优模型呢？

这个问题可以从概率论的角度阐释。线性回归得到的是统计意义上的拟合结果，在单变量的情形下，可能每一个样本点都没有落在求得的直线上。

对这个现象的一种解释是回归结果可以完美匹配理想样本点的分布，但训练中使用的真实样本点是理想样本点和噪声叠加的结果，因而与回归模型之间产生了偏差，而每个样本点上噪声的取值就等于 $y_k−f\left (x_k\right )$。

假定影响样本点的噪声满足参数为 $\left (0,σ^2\right )$ 的正态分布，对参数 w 的推导可以用最大似然的方式进行，即在已知样本数据及其分布的条件下，找到使样本数据以最大概率出现的假设。

单个样本$\left (x_k, y_k\right )$ 出现的概率实际上就是噪声等于 $y_k−f\left (x_k\right )$ 的概率，而相互独立的所有样本同时出现的概率则是每个样本出现概率的乘积，其表达式可以写成
$$
p\left (x_1,x2, \cdots,x_k,\cdots|w \right )=\\
\prod_{k}{\frac{1}{\sqrt{2\pi}\sigma }\exp\left [ -\frac{1}{2\sigma^2}\left ( y_k-w^Tx_k\right )^2\right ]}
$$


而最大似然估计的任务就是让以上表达式的取值最大化。出于计算简便的考虑，上面的乘积式可以通过取对数的方式转化成求和式，且取对数的操作并不会影响其单调性。经过换算后，上式的最大化就可以等效为 $\sum_{k=1}{\left ( y_{k}-w^{T}x_{k}\right )}^{2}$ 的最小化。

因此，**对于单变量线性回归而言，在误差函数服从正态分布的情况下，从几何意义出发的最小二乘法与从概率意义出发的最大似然估计是等价的**。

## 二、多元线性回归的最优参数

多元线性回归中的参数 w 也可以用最小二乘法进行估计，其最优解同样用偏导数确定，但参与运算的元素从向量变成了矩阵。在理想的情况下，多元线性回归的最优参数为
$$
w^{*}=\left ( X^{T}X\right )^{-1}X^{T}y
$$
式中的 $X$ 是由所有样本 $x=(x_0,x_1,x_2,\cdots,x_n)$ 的转置共同构成的矩阵。但这一表达式只在矩阵 $(X^TX)$ 的逆矩阵存在时成立。在大量复杂的实际任务中，每个样本中属性的数目甚至会超过训练集中的样本总数，此时求出的最优解 $w^{∗}$ 就不是唯一的，解的选择将依赖于学习算法的归纳偏好。